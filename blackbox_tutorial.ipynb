{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-box attack Tutorial\n",
    "\n",
    "In this tutorial, you will learn how to apply black-box attacks, using the Section Injection practical manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:02:07.399357Z",
     "start_time": "2023-10-20T07:02:05.614046Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import magic\n",
    "import numpy as np\n",
    "from secml.array import CArray\n",
    "\n",
    "from secml_malware.attack.blackbox.c_wrapper_phi import CEnd2EndWrapperPhi\n",
    "from secml_malware.attack.blackbox.ga.c_base_genetic_engine import CGeneticAlgorithm\n",
    "from secml_malware.models.c_classifier_end2end_malware import CClassifierEnd2EndMalware\n",
    "from secml_malware.models.malconv import MalConv\n",
    "\n",
    "net = CClassifierEnd2EndMalware(MalConv())\n",
    "net.load_pretrained_model()\n",
    "net = CEnd2EndWrapperPhi(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have created the network (MalConv) and it has been passed wrapped with a *CClassifierEnd2EndMalware* model class.\n",
    "This object generalizes PyTorch end-to-end ML models.\n",
    "Since MalConv is already coded inside the plugin, the weights are also stored, and they can be retrieved with the *load_pretrained_model* method.\n",
    "\n",
    "If you wish to use diffierent weights, pass the path to the PyTorch *pth* file to that method.\n",
    "\n",
    "Then, we wrap it inside a `CEnd2EndWrapperPhi`, that is an interface that abstracts from the feature extraction phase of the model.\n",
    "This is needed for the black-box settings of the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:02:07.548857Z",
     "start_time": "2023-10-20T07:02:07.400527Z"
    }
   },
   "outputs": [],
   "source": [
    "from secml_malware.attack.blackbox.c_gamma_sections_evasion import CGammaSectionsEvasionProblem\n",
    "goodware_folder = 'secml_malware/data/goodware_samples' #INSERT GOODWARE IN THAT FOLDER\n",
    "section_population, what_from_who = CGammaSectionsEvasionProblem.create_section_population_from_folder(goodware_folder, how_many=10, sections_to_extract=['.rdata'])\n",
    "\n",
    "attack = CGammaSectionsEvasionProblem(section_population, net, population_size=10, penalty_regularizer=1e-6, iterations=10, threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the section injection attack implemented with GAMMA, we need first to extract the goodware sections.\n",
    "Then, we create a `CGammaSectionsEvasionProblem` object, that contains the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:02:07.658018Z",
     "start_time": "2023-10-20T07:02:07.550365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Added p2.file with confidence 0.9112256765365601\n",
      "> Added petya.file with confidence 0.9112256765365601\n",
      "> Added p1.file with confidence 0.9112256765365601\n"
     ]
    }
   ],
   "source": [
    "folder = 'secml_malware/data/malware_samples/test_folder'  #INSERT MALWARE IN THAT FOLDER\n",
    "X = []\n",
    "y = []\n",
    "file_names = []\n",
    "for i, f in enumerate(os.listdir(folder)):\n",
    "    path = os.path.join(folder, f)\n",
    "    if \"PE32\" not in magic.from_file(path):\n",
    "        continue\n",
    "    with open(path, \"rb\") as file_handle:\n",
    "        code = file_handle.read()\n",
    "    x = CArray(np.frombuffer(code, dtype=np.uint8)).atleast_2d()\n",
    "    _, confidence = net.predict(x, True)\n",
    "\n",
    "    if confidence[0, 1].item() < 0.5:\n",
    "        continue\n",
    "\n",
    "    print(f\"> Added {f} with confidence {confidence[0,1].item()}\")\n",
    "    X.append(x)\n",
    "    conf = confidence[1][0].item()\n",
    "    y.append([1 - conf, conf])\n",
    "    file_names.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a simple dataset from the `malware_samples/test_folder` that you have filled with malware to test the attacks.\n",
    "We discard all the samples that are not seen by the network.\n",
    "The `CArray` class is the base object you will handle when dealing with vectors in this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:02:28.789255Z",
     "start_time": "2023-10-20T07:02:07.656957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9112256765365601, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433, 0.002221981529146433]\n",
      "0.002221981529146433\n",
      "[0.9112256765365601, 0.031418055295944214, 0.031418055295944214, 0.05943218246102333, 0.027438974007964134, 0.027438974007964134, 0.027438974007964134, 0.03099643439054489, 0.03099643439054489, 0.023107970133423805, 0.023107970133423805, 0.023107970133423805]\n",
      "0.023107970133423805\n",
      "[0.9112256765365601, 0.02237715944647789, 0.036044590175151825, 0.036044590175151825, 0.03532141447067261, 0.026757843792438507, 0.026757843792438507, 0.026757843792438507, 0.021750085055828094, 0.021750085055828094, 0.021750085055828094, 0.019729789346456528]\n",
      "0.019729789346456528\n"
     ]
    }
   ],
   "source": [
    "engine = CGeneticAlgorithm(attack)\n",
    "for sample, label in zip(X, y):\n",
    "    y_pred, adv_score, adv_ds, f_obj = engine.run(sample, CArray(label[1]))\n",
    "    print(engine.confidences_)\n",
    "    print(f_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `adv_ds` object, you can find the adversarial example computed by the attack.\n",
    "You can reconstruct the functioning example by using a specific function inside the plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:02:28.905157Z",
     "start_time": "2023-10-20T07:02:28.849615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026793455705046654\n"
     ]
    }
   ],
   "source": [
    "adv_x = adv_ds.X[0,:]\n",
    "engine.write_adv_to_file(adv_x,'adv_exe')\n",
    "with open('adv_exe', 'rb') as h:\n",
    "    code = h.read()\n",
    "real_adv_x = CArray(np.frombuffer(code, dtype=np.uint8))\n",
    "_, confidence = net.predict(CArray(real_adv_x), True)\n",
    "print(confidence[0,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and you're done!\n",
    "If you want to create a real sample (stored on disk), just have a look at the `create_real_sample_from_adv` of each attack. It accepts a third string argument that will be used as a destination file path for storing the adversarial example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
